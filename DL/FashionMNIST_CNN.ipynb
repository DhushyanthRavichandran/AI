{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkcxATX0IpAVmPZ0LXE9VW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhushyanthRavichandran/AI/blob/main/DL/FashionMNIST_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# *Fashion MNIST*\n",
        " It consists of 70,000 grayscale images of 28x28 pixels each, representing various fashion items like T-shirts, dresses, shoes, and so on. The dataset is divided into 60,000 training images and 10,000 testing images, with 10 different classes"
      ],
      "metadata": {
        "id": "z7Tm802TBWLx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rI7NR7p16jQm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras import layers,models\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3Zv52iW7bh7",
        "outputId": "c3ef69db-3625-4c9e-9a2c-c9f85cf6b404"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test=x_train/255.0,x_test/255.0"
      ],
      "metadata": {
        "id": "LcDBosSv7yMZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images=x_train.reshape((60000,28,28,1))\n",
        "test_images=x_test.reshape((10000,28,28,1))\n",
        "# Convert labels to one-hot encoding\n",
        "train_labels = to_categorical(y_train)\n",
        "test_labels = to_categorical(y_test)\n"
      ],
      "metadata": {
        "id": "9vgHMSdw8hF9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=models.Sequential()\n",
        "model.add(layers.Conv2D(32,(3,3),activation='relu',input_shape=(28,28,1)))#filtersize=>(3,3),and 32 filters\n",
        "model.add(layers.MaxPool2D((2,2)))\n",
        "model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
        "model.add(layers.MaxPool2D((2,2)))\n",
        "model.add(layers.Conv2D(64,(3,3),activation='relu'))"
      ],
      "metadata": {
        "id": "3Mxc3RbA9jMc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64,activation='relu'))\n",
        "model.add(layers.Dense(10,activation='softmax'))"
      ],
      "metadata": {
        "id": "KXLESYlE_A-4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "3haJon6P_lof"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDUcsrhh_z5b",
        "outputId": "21eeb4fd-6297-4d8b-c6aa-1ed753522b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 76s 40ms/step - loss: 0.5032 - accuracy: 0.8165 - val_loss: 0.3652 - val_accuracy: 0.8689\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 0.3260 - accuracy: 0.8819 - val_loss: 0.3394 - val_accuracy: 0.8776\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 72s 39ms/step - loss: 0.2783 - accuracy: 0.8981 - val_loss: 0.2933 - val_accuracy: 0.8916\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.2502 - accuracy: 0.9085 - val_loss: 0.2822 - val_accuracy: 0.8940\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.2283 - accuracy: 0.9155 - val_loss: 0.2736 - val_accuracy: 0.9017\n",
            "Epoch 6/10\n",
            " 323/1875 [====>.........................] - ETA: 50s - loss: 0.1933 - accuracy: 0.9266"
          ]
        }
      ]
    }
  ]
}